<a class="anchor" id="0"></a>
# **머신러닝 요약**

**2023년 1학기 중간고사 머신러닝 내용 공부**.

<a class="anchor" id="0.1"></a>

# 목차
[1. 머신러닝이란?](#1)

## 1.1 머신 러닝이란?<a class="anchor" id="1"></a>

머신러닝 = 컴퓨터 스스로 학습 능력을 갖도록 하는 연구 분야


    * 작업 T = 새로운 메일의 스팸 여부 판단
    * 경험 E = 훈련 데이터
    * 성능 P = 스팸 여부 판단의 정확도

데어터 셋 용어

    훈련셋 : 머신러닝 프로그램이 훈련하는데 사용하는 데이터 집합
    샘플   : 각각의 훈련 데이터
    
## 1.2 머신러닝 시스템 유형

머신러닝 시스템의 유형을 다양한 기준으로 분류할 수 있다.

1. 훈련 지도 여부

지도 학습

비지도 학습

준지도 학습

자기주도 학습

강화 학습

2. 실시간 훈련 여부

배치 학습

온라인 학습

3. 예측 모델 사용 여부

사례 기반 학습

모델 기반 학습


     
# 4월 17일 수업 내용

## 다항 회귀

이론적으론 좋다 하지만 몇차 함수를 넣어줘야 할지 모른다. 그리고 차수가 많아지면 너무 수가 커진다.
그래서 쓸모가 없다.

## 학습 곡선

## 학습 곡선이란?

    훈련을 얼마나 잘 하고 있는 평가하는 방식중 하나 곡선을 보고서
    훈련중 규제를 가하는데 더 좋은 결과를 나오게 한다.

![image](https://user-images.githubusercontent.com/112872986/232418981-b3ff9947-d7f3-4442-94e8-0b70c0782d4f.png)

300차 함수 그래프는 최대한 값을 예측하지만 이 그래프에만 적합하다. 그래서 다른 곳에서 쓸모가 없다.
새로운 값이 기존 값과 차이가 크다 그래서 실전에서 문제가 생긴다.

함수가 커지면 분산이 커진다

편향= 얼마나 잘 예측하지 못하는가 차수가 낮으면 편향이 크다.

자유도 = 모뎅이 얼마나 복잡한가 = 파라미터의 개수

### 과소 적합 vs 과대 적합

과소 적합 편향 높음 

과대적합은 정확한 예측 불가능 값이 크기 때문에 실전에서 잘 할 수 있도록 잘 가르쳐야하는데 
이때 규제를 하는 것이다.

교차검증

### 학습곡선

![image](https://user-images.githubusercontent.com/112872986/232422540-8a164001-fec2-4737-b2d0-ff5ca1e238ce.png)

x= 100개 1,3,5,7,10 1개로 훈련하고 검증 3개로 훈련하고 검증...... 10개로 훈련하고 검증
훈련셋의 크기가 증가할 수록 검증 점수는 낮아진다

* 과소적합은  선이 안 내려온다

![image](https://user-images.githubusercontent.com/112872986/232423500-86d720c0-d945-4ff3-a390-4bd284a26c70.png)

* 과대적합 모델은 빨간 선은 조금 씩 내려간다 검증 점수(파란선)은 어느 순간부터 정채하거나 올라갈 수도 있다.
* 훈련 점수와 검증 점수 사이의 차이가 어느 순간부터 줄어들지 않는다

### 모델 일반화

-훈련 후 새로운 데이터에 대한 예측에서 오차 발생

발생 원인
    * 편향
    * 분산
    * 줄일 수 없는차오차

기본적으로 교차검증이 편하다.

## 모델 훈련

### 자유도와 규제
* 자유도 : 학습 모델 결정에 영향을 주는 요소들의 수
    * 선형회귀  : 특성 수
    * 다항 회귀 : 특성 수 + 차수
* 규제 : 자유도 제한
    * 선형 회귀 모델 규제 : 가중치 역활 제한 
    
       (세타) 값들을 작게 유지해야한다=세타의 절대값 줄이기   (x 값들은 그냥 수임)

    * 다항 회귀 모델 규제 : 차수 줄이기=> 세타의 개수 줄이기

        필요없는 특성은 제거한다
      

## 릿지 회귀

![image](https://user-images.githubusercontent.com/112872986/232431845-baa1e280-8c2c-451c-81b3-bd9ea3442f3a.png)


* 모델은 세타의 제곱 값을 줄이려 한다.
* 알파가 클수록 규제가 쎄다

## 라쏘 회귀
* 비용함수

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>J</mi>
  <mo stretchy="false">(</mo>
  <mi>&#x3B8;</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mtext>MSE</mtext>
  <mo stretchy="false">(</mo>
  <mi>&#x3B8;</mi>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <mn>2</mn>
  <mi>&#x3B1;</mi>
  <mstyle scriptlevel="0">
    <mspace width="0.167em"></mspace>
  </mstyle>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>n</mi>
    </mrow>
  </munderover>
  <mo>&#x2223;</mo>
  <msub>
    <mi>&#x3B8;</mi>
    <mi>i</mi>
  </msub>
  <mo>&#x2223;</mo>
</math>

알파는 규제 강도다.

![image](https://user-images.githubusercontent.com/112872986/232432642-07f120b1-f93b-49fd-be3e-533ce7545055.png)

규제를 강하게 주면 차수가 낮아진다.

일부 세타 i 값을 0으로 만들어준다  즉. 일부 특성을 무시하게 한다.

세타를 미분하면 1 또는 -1이다.

라쏘는 세타를 0에 수렵하게 만들려고 한다.


 
## 엘라스틱 회귀
- 엘라스틱= 라쏘 + 릿지

* 비용함수
<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>J</mi>
  <mo stretchy="false">(</mo>
  <mi>&#x3B8;</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mtext>MSE</mtext>
  <mo stretchy="false">(</mo>
  <mi>&#x3B8;</mi>
  <mo stretchy="false">)</mo>
  <mo>+</mo>
  <mi>r</mi>
  <mo>&#x22C5;</mo>
  <mrow data-mjx-texclass="ORD">
    <mo minsize="2.047em" maxsize="2.047em">(</mo>
  </mrow>
  <mn>2</mn>
  <mi>&#x3B1;</mi>
  <mstyle scriptlevel="0">
    <mspace width="0.167em"></mspace>
  </mstyle>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>n</mi>
    </mrow>
  </munderover>
  <mo>&#x2223;</mo>
  <msub>
    <mi>&#x3B8;</mi>
    <mi>i</mi>
  </msub>
  <mo>&#x2223;</mo>
  <mrow data-mjx-texclass="ORD">
    <mo minsize="2.047em" maxsize="2.047em">)</mo>
  </mrow>
  <mo>+</mo>
  <mo stretchy="false">(</mo>
  <mn>1</mn>
  <mo>&#x2212;</mo>
  <mi>r</mi>
  <mo stretchy="false">)</mo>
  <mo>&#x22C5;</mo>
  <mrow data-mjx-texclass="ORD">
    <mo minsize="2.047em" maxsize="2.047em">(</mo>
  </mrow>
  <mfrac>
    <mi>&#x3B1;</mi>
    <msub>
      <mi>m</mi>
      <mi>b</mi>
    </msub>
  </mfrac>
  <mstyle scriptlevel="0">
    <mspace width="0.167em"></mspace>
  </mstyle>
  <munderover>
    <mo data-mjx-texclass="OP">&#x2211;</mo>
    <mrow data-mjx-texclass="ORD">
      <mi>i</mi>
      <mo>=</mo>
      <mn>1</mn>
    </mrow>
    <mrow data-mjx-texclass="ORD">
      <mi>n</mi>
    </mrow>
  </munderover>
  <msubsup>
    <mi>&#x3B8;</mi>
    <mi>i</mi>
    <mn>2</mn>
  </msubsup>
  <mrow data-mjx-texclass="ORD">
    <mo minsize="2.047em" maxsize="2.047em">)</mo>
  </mrow>
</math>

# 4월 18일 공부내용

숫자 2의 28*28 픽셀 중에서 중앙에 있는 숫자의 픽셀만 필요하다 중앙에 있는 픽셀(세타)만 저장하고 나머지는 0으로 하면 세타갯수가 줄어든다.

## 조기 종료

* 검증셋에 대한 비용함수의 값이 줄어들다가 다시 커지는 순간 훈련을 종료한다.

# 로지스틱 회귀 

회귀 모델을 분류 모델로 활용할 수 있다.

## 확률 추정

시그모이드 함수 s 자 모양의 그래프 식보다 그래프에 중점으로

t=0 일때 0.5 를 가지며 보다 크면 1에 수렴 작으면 -1에 수렴한다


## 불꽃 데이
![image](https://user-images.githubusercontent.com/112872986/232687666-c2c5be2d-9f78-4808-a3e1-0e9e971ba66f.png)

꽃입의 너비 기준으
![image](https://user-images.githubusercontent.com/112872986/232688098-8cc166f6-dbd5-4070-9bc0-0700741fe4f4.png)

1.65보다 크면 버즈니카 작으면 아니다.
![image](https://user-images.githubusercontent.com/112872986/232688813-df8eff4b-890e-40aa-87e8-4954aa6c1b14.png)


전처리 과정 다시 보
결정 함수값이 임계값으로 변하는가

4장
경사하강법 
선형회기 파라미터를 어떻게 찾는가

작동하는 요령

# 6장 결정트리

결정트리 =  분류모델

장점 : 전처리가 불필요 (필요한 경우도 존재)

기준을 어떻게 잡냐 

다해본다 꽃입의 길이 150개 다 해보고 넓이 150개 다 비교해보고

150개를 다 분류 할 때 마다 gini(불순도)를 계산해서 가장 작은 값의 gini의 분류 값을 사용함

CART 분류 아로리즘을 통해서 gini 값을 더 낮은 것을 사용함









